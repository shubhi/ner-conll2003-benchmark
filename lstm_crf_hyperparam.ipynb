{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9462e83a-fdaa-48a9-b8c6-97e334e60e96",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM & CRF (Hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36bdc40-8d3b-411f-89f4-07f13e416513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from data.ner_dataset import NERDataset\n",
    "from models.lstm_crf import BiLSTMCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e0b5d-69a3-4597-90e3-5d84e695710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=\"training.log\",\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e09ee1-adc5-4ab1-b99a-45d3f0adfaef",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c4200-83d2-4683-a323-cba401ec6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70cf43-563e-4e4c-bb4a-4110af16652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "unique_labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba71d0d-9827-4bda-99dc-de77b2dc8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(dataset[\"train\"], tokenizer)\n",
    "val_dataset = NERDataset(dataset[\"validation\"], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9e679-0131-414c-875f-ac9f69d3e59e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6836e-da16-4ef6-835a-98bafffc8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LABELS = len(unique_labels)\n",
    "PAD_IDX = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef05520-de0a-4623-b886-a89c69f2b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMCRF(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LABELS, PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d8981-d724-4a9c-b59f-95770a4ded72",
   "metadata": {},
   "source": [
    "## Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6826e-dcef-4db2-96a0-e9d54dff30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, label_list):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            # Compute loss if labels are provided\n",
    "            loss = model(input_ids, attention_mask, labels)\n",
    "            if loss is not None:\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "            # Decode predictions\n",
    "            preds = model(input_ids, attention_mask)\n",
    "\n",
    "            # Align predictions and true labels\n",
    "            for pred, label, mask in zip(preds, labels.cpu().numpy(), attention_mask.cpu().numpy()):\n",
    "                true_seq = [label_list[l] for l, m in zip(label, mask) if m == 1 and l != -100]\n",
    "                pred_seq = [label_list[p] for p, m in zip(pred, mask) if m == 1][: len(true_seq)]\n",
    "                if len(pred_seq) == len(true_seq):\n",
    "                    true_labels.append(true_seq)\n",
    "                    predictions.append(pred_seq)\n",
    "\n",
    "    # Flatten predictions and labels for evaluation\n",
    "    flattened_preds = [label for seq in predictions for label in seq]\n",
    "    flattened_labels = [label for seq in true_labels for label in seq]\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(flattened_labels, flattened_preds)\n",
    "    f1 = f1_score(flattened_labels, flattened_preds, average=\"weighted\")\n",
    "    classification_rep = classification_report(\n",
    "        flattened_labels, flattened_preds, target_names=label_list, zero_division=1\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_val_loss / len(loader) if len(loader) > 0 else None,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"classification_report\": classification_rep,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e74d7-8fec-4d60-a0f2-a4a0ba90ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model, train_loader, val_loader, optimizer, label_list, epochs, device, print_every=25, lr=None, dropout=None\n",
    "):\n",
    "    # Track metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience, epochs_without_improvement = 10, 0\n",
    "\n",
    "    # Create results directory\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.makedirs(\"results\")\n",
    "\n",
    "    epoch_start_time = time.time()  # Track overall epoch timing\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss, correct_train_preds, total_train_tokens = 0, 0, 0\n",
    "        for input_ids, attention_mask, labels in train_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(input_ids, attention_mask, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Perform evaluation and log metrics only every print_every epochs\n",
    "        if (epoch + 1) % print_every == 0 or epoch == epochs - 1:\n",
    "            model.eval()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            correct_train_preds, total_train_tokens = 0, 0\n",
    "            for input_ids, attention_mask, labels in train_loader:\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "                with torch.no_grad():\n",
    "                    preds = model(input_ids, attention_mask)\n",
    "                    for pred, label, mask in zip(preds, labels.cpu().numpy(), attention_mask.cpu().numpy()):\n",
    "                        true_seq = [label_list[l] for l, m in zip(label, mask) if m == 1 and l != -100]\n",
    "                        pred_seq = [label_list[p] for p, m in zip(pred, mask) if m == 1][: len(true_seq)]\n",
    "                        correct_train_preds += sum([p == t for p, t in zip(pred_seq, true_seq)])\n",
    "                        total_train_tokens += len(true_seq)\n",
    "\n",
    "            train_accuracy = correct_train_preds / total_train_tokens if total_train_tokens > 0 else 0\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "            # Perform validation evaluation\n",
    "            with torch.no_grad():\n",
    "                results = evaluate(model, val_loader, label_list)\n",
    "                avg_val_loss = results[\"loss\"]\n",
    "                val_losses.append(avg_val_loss)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            correct_val_preds, total_val_tokens = 0, 0\n",
    "            for input_ids, attention_mask, labels in val_loader:\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "                with torch.no_grad():\n",
    "                    preds = model(input_ids, attention_mask)\n",
    "                    for pred, label, mask in zip(preds, labels.cpu().numpy(), attention_mask.cpu().numpy()):\n",
    "                        true_seq = [label_list[l] for l, m in zip(label, mask) if m == 1 and l != -100]\n",
    "                        pred_seq = [label_list[p] for p, m in zip(pred, mask) if m == 1][: len(true_seq)]\n",
    "                        correct_val_preds += sum([p == t for p, t in zip(pred_seq, true_seq)])\n",
    "                        total_val_tokens += len(true_seq)\n",
    "\n",
    "            val_accuracy = correct_val_preds / total_val_tokens if total_val_tokens > 0 else 0\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Log metrics\n",
    "            elapsed_time = time.time() - epoch_start_time  # Calculate elapsed time\n",
    "            logger.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            logger.info(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "            logger.info(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            logger.info(f\"Time elapsed for last {print_every} epoch(s): {elapsed_time:.2f} seconds\")\n",
    "            epoch_start_time = time.time()  # Reset timer for next interval\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss - 0.001:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    logger.info(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    # Save losses\n",
    "    losses_df = pd.DataFrame(\n",
    "        {\n",
    "            \"epoch\": list(range(1, len(train_losses) + 1)),\n",
    "            \"train_loss\": train_losses,\n",
    "            \"val_loss\": [None] * (len(train_losses) - len(val_losses)) + val_losses,\n",
    "        }\n",
    "    )\n",
    "    losses_path = os.path.join(\"results\", f\"losses_lr_{lr}_dropout_{dropout}.csv\")\n",
    "    losses_df.to_csv(losses_path, index=False)\n",
    "\n",
    "    # Save accuracies\n",
    "    accuracies_df = pd.DataFrame(\n",
    "        {\n",
    "            \"epoch\": list(range(1, len(train_accuracies) + 1)),\n",
    "            \"train_accuracy\": train_accuracies,\n",
    "            \"val_accuracy\": [None] * (len(train_accuracies) - len(val_accuracies)) + val_accuracies,\n",
    "        }\n",
    "    )\n",
    "    accuracies_path = os.path.join(\"results\", f\"accuracies_lr_{lr}_dropout_{dropout}.csv\")\n",
    "    accuracies_df.to_csv(accuracies_path, index=False)\n",
    "\n",
    "    return min(val_losses), max(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fad40-7225-4e37-b960-3638568d0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "dropouts = [0.1, 0.2]\n",
    "hyperparams = list(itertools.product(learning_rates, dropouts))\n",
    "\n",
    "# Results storage\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62689641-1c04-4d46-a8eb-ca83f7c63f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr, dropout in hyperparams:\n",
    "    logger.info(f\"Training with Learning Rate: {lr}, Dropout: {dropout}\")\n",
    "\n",
    "    # Initialize model, optimizer, and data loaders\n",
    "    model = BiLSTMCRF(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_dim=128,\n",
    "        hidden_dim=256,\n",
    "        num_labels=len(unique_labels),\n",
    "        pad_idx=tokenizer.pad_token_id,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4000, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4000)\n",
    "\n",
    "    # Train and validate\n",
    "    best_val_loss, best_val_accuracy = train_and_validate(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        unique_labels,\n",
    "        epochs=1000,\n",
    "        device=device,\n",
    "        print_every=50,\n",
    "        lr=lr,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "    # Log and store results\n",
    "    logger.info(f\"Best Validation Loss for LR: {lr}, Dropout: {dropout}: {best_val_loss:.4f}\")\n",
    "    logger.info(f\"Best Validation Accuracy for LR: {lr}, Dropout: {dropout}: {best_val_accuracy:.4f}\")\n",
    "    results.append(\n",
    "        {\n",
    "            \"learning_rate\": lr,\n",
    "            \"dropout\": dropout,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"best_val_accuracy\": best_val_accuracy,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Save hyperparameter results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"results/hyperparameter_results.csv\", index=False)\n",
    "\n",
    "logger.info(\"Hyperparameter tuning complete. Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be81a3-116c-4361-b4dd-3f147859c1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
