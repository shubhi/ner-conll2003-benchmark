{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9462e83a-fdaa-48a9-b8c6-97e334e60e96",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM & CRF (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36bdc40-8d3b-411f-89f4-07f13e416513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from data.ner_dataset import NERDataset\n",
    "from models.lstm_crf import BiLSTMCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e0b5d-69a3-4597-90e3-5d84e695710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=\"training.log\",\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e09ee1-adc5-4ab1-b99a-45d3f0adfaef",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c4200-83d2-4683-a323-cba401ec6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70cf43-563e-4e4c-bb4a-4110af16652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "unique_labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba71d0d-9827-4bda-99dc-de77b2dc8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(dataset[\"train\"], tokenizer)\n",
    "val_dataset = NERDataset(dataset[\"validation\"], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9e679-0131-414c-875f-ac9f69d3e59e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6836e-da16-4ef6-835a-98bafffc8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LABELS = len(unique_labels)\n",
    "PAD_IDX = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef05520-de0a-4623-b886-a89c69f2b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d8981-d724-4a9c-b59f-95770a4ded72",
   "metadata": {},
   "source": [
    "## Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6826e-dcef-4db2-96a0-e9d54dff30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, label_list):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            # Compute loss if labels are provided\n",
    "            loss = model(input_ids, attention_mask, labels)\n",
    "            if loss is not None:\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "            # Decode predictions\n",
    "            preds = model(input_ids, attention_mask)\n",
    "\n",
    "            # Align predictions and true labels\n",
    "            for pred, label, mask in zip(preds, labels.cpu().numpy(), attention_mask.cpu().numpy()):\n",
    "                true_seq = [label_list[l] for l, m in zip(label, mask) if m == 1 and l != -100]\n",
    "                pred_seq = [label_list[p] for p, m in zip(pred, mask) if m == 1][: len(true_seq)]\n",
    "                if len(pred_seq) == len(true_seq):\n",
    "                    true_labels.append(true_seq)\n",
    "                    predictions.append(pred_seq)\n",
    "\n",
    "    # Flatten predictions and labels for evaluation\n",
    "    flattened_preds = [label for seq in predictions for label in seq]\n",
    "    flattened_labels = [label for seq in true_labels for label in seq]\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(flattened_labels, flattened_preds)\n",
    "    f1 = f1_score(flattened_labels, flattened_preds, average=\"weighted\")\n",
    "    classification_rep = classification_report(\n",
    "        flattened_labels, flattened_preds, target_names=label_list, zero_division=1\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_val_loss / len(loader) if len(loader) > 0 else None,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"classification_report\": classification_rep,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e74d7-8fec-4d60-a0f2-a4a0ba90ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model, train_loader, val_loader, optimizer, label_list, epochs, device, print_every=25, lr=None, dropout=None\n",
    "):\n",
    "    # Track metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    # Create results directory\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.makedirs(\"results\")\n",
    "\n",
    "    epoch_start_time = time.time()  # Track overall epoch timing\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss, correct_train_preds, total_train_tokens = 0, 0, 0\n",
    "        for input_ids, attention_mask, labels in train_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(input_ids, attention_mask, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Perform evaluation and log metrics only every print_every epochs\n",
    "        if (epoch + 1) % print_every == 0 or epoch == epochs - 1:\n",
    "            model.eval()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            correct_train_preds, total_train_tokens = 0, 0\n",
    "            for input_ids, attention_mask, labels in train_loader:\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "                with torch.no_grad():\n",
    "                    preds = model(input_ids, attention_mask)\n",
    "                    for pred, label, mask in zip(preds, labels.cpu().numpy(), attention_mask.cpu().numpy()):\n",
    "                        true_seq = [label_list[l] for l, m in zip(label, mask) if m == 1 and l != -100]\n",
    "                        pred_seq = [label_list[p] for p, m in zip(pred, mask) if m == 1][: len(true_seq)]\n",
    "                        correct_train_preds += sum([p == t for p, t in zip(pred_seq, true_seq)])\n",
    "                        total_train_tokens += len(true_seq)\n",
    "\n",
    "            train_accuracy = correct_train_preds / total_train_tokens if total_train_tokens > 0 else 0\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "            # Perform validation evaluation\n",
    "            with torch.no_grad():\n",
    "                results = evaluate(model, val_loader, label_list)\n",
    "                avg_val_loss = results[\"loss\"]\n",
    "                val_losses.append(avg_val_loss)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            correct_val_preds, total_val_tokens = 0, 0\n",
    "            for input_ids, attention_mask, labels in val_loader:\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "                with torch.no_grad():\n",
    "                    preds = model(input_ids, attention_mask)\n",
    "                    for pred, label, mask in zip(preds, labels.cpu().numpy(), attention_mask.cpu().numpy()):\n",
    "                        true_seq = [label_list[l] for l, m in zip(label, mask) if m == 1 and l != -100]\n",
    "                        pred_seq = [label_list[p] for p, m in zip(pred, mask) if m == 1][: len(true_seq)]\n",
    "                        correct_val_preds += sum([p == t for p, t in zip(pred_seq, true_seq)])\n",
    "                        total_val_tokens += len(true_seq)\n",
    "\n",
    "            val_accuracy = correct_val_preds / total_val_tokens if total_val_tokens > 0 else 0\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Log metrics\n",
    "            elapsed_time = time.time() - epoch_start_time  # Calculate elapsed time\n",
    "            logger.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            logger.info(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "            logger.info(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            logger.info(f\"Time elapsed for last {print_every} epoch(s): {elapsed_time:.2f} seconds\")\n",
    "            epoch_start_time = time.time()  # Reset timer for next interval\n",
    "\n",
    "    # Save training losses\n",
    "    train_losses_df = pd.DataFrame(\n",
    "        {\n",
    "            \"train_loss\": train_losses,\n",
    "        }\n",
    "    )\n",
    "    train_losses_path = os.path.join(\"results\", f\"train_losses_lr_{lr}_dropout_{dropout}.csv\")\n",
    "    train_losses_df.to_csv(train_losses_path, index=False)\n",
    "\n",
    "    # Save validation losses\n",
    "    val_losses_df = pd.DataFrame(\n",
    "        {\n",
    "            \"val_loss\": val_losses,\n",
    "        }\n",
    "    )\n",
    "    val_losses_path = os.path.join(\"results\", f\"val_losses_lr_{lr}_dropout_{dropout}.csv\")\n",
    "    val_losses_df.to_csv(val_losses_path, index=False)\n",
    "\n",
    "    # Save training accuracies\n",
    "    train_accuracies_df = pd.DataFrame(\n",
    "        {\n",
    "            \"train_accuracy\": train_accuracies,\n",
    "        }\n",
    "    )\n",
    "    train_accuracies_path = os.path.join(\"results\", f\"train_accuracies_lr_{lr}_dropout_{dropout}.csv\")\n",
    "    train_accuracies_df.to_csv(train_accuracies_path, index=False)\n",
    "\n",
    "    # Save validation accuracies\n",
    "    val_accuracies_df = pd.DataFrame(\n",
    "        {\n",
    "            \"val_accuracy\": val_accuracies,\n",
    "        }\n",
    "    )\n",
    "    val_accuracies_path = os.path.join(\"results\", f\"val_accuracies_lr_{lr}_dropout_{dropout}.csv\")\n",
    "    val_accuracies_df.to_csv(val_accuracies_path, index=False)\n",
    "\n",
    "    return min(val_losses), max(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e89d86-95f3-4977-a146-fa37a6728e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "dropout = 0.25\n",
    "\n",
    "model = BiLSTMCRF(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_labels=len(unique_labels),\n",
    "    pad_idx=tokenizer.pad_token_id,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4000, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62689641-1c04-4d46-a8eb-ca83f7c63f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss, best_val_accuracy = train_and_validate(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    unique_labels,\n",
    "    epochs=450,\n",
    "    device=device,\n",
    "    print_every=25,\n",
    "    lr=lr,\n",
    "    dropout=dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d9af6-52ee-4c19-8680-f9c19aca866e",
   "metadata": {},
   "source": [
    "## Evaluation on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d43403-5449-4355-bcd1-aa2f47b64971",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = NERDataset(dataset[\"test\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa20294b-2c2d-40c6-b7f8-ddcffc9f45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate(model, test_loader, unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb3a2e-fa1e-4470-a34c-c9d1de09f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set Metrics:\")\n",
    "print(f\"Loss: {test_results['loss']:.4f}\")\n",
    "print(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['f1']:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(test_results[\"classification_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcaabce-8fa6-43ab-ba34-891a5b95d0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
