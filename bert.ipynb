{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acae0990-87ac-4d98-824f-1e0484016b89",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02946a1b-93e1-4ef3-9d9b-439c4b8a0805",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087816c-b10a-4b2b-b43a-e346e95061d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524320f-82a8-4e70-9793-d9f849b1fdd0",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8721f-d7d6-45b2-aa84-bf5c21bddeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CoNLL-2003 dataset\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c8bea-85cf-4159-8501-6b255406e860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "# Tokenization and alignment\n",
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized_inputs = tokenizer(batch[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(batch[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
    "        labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb22191-cd4f-4a35-83ee-fd0ea7369747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DataLoader collate function\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(x[\"input_ids\"]) for x in batch]\n",
    "    attention_mask = [torch.tensor(x[\"attention_mask\"]) for x in batch]\n",
    "    labels = [torch.tensor(x[\"labels\"]) for x in batch]\n",
    "\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"labels\": labels_padded,\n",
    "    }\n",
    "\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=128, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(tokenized_datasets[\"validation\"], batch_size=128, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b791787-b611-4105-b233-be1a62733e77",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9aa2f4-ce43-449f-8b8d-e6bbaa2dc19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertForNERWithLayerAttention(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_labels):\n",
    "        super(BertForNERWithLayerAttention, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
    "        self.num_hidden_layers = self.bert.config.num_hidden_layers + 1\n",
    "        self.layer_weights = nn.Parameter(torch.ones(self.num_hidden_layers))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = torch.stack(outputs.hidden_states, dim=0)  # (num_layers, batch_size, seq_len, hidden_size)\n",
    "        weighted_hidden_states = torch.sum(self.layer_weights[:, None, None, None] * hidden_states, dim=0)\n",
    "        sequence_output = self.dropout(weighted_hidden_states)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, logits.size(-1))[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss = loss_fn(active_logits, active_labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b3395d-c395-4195-981f-e61e0023357b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForNERWithLayerAttention(pretrained_model_name=\"bert-base-cased\", num_labels=num_labels).to(device)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045508ff-e5c7-4dcf-a18b-94f44e7a5e6e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb4fcc-3879-4216-9191-13d6076edafb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1} Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ced811-b47d-4ced-8dae-5bacd5c53c9e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76f97b-1bed-4563-af48-3a2f58660181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[\"logits\"]\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "            predictions.append([label_list[p] for p, l in zip(pred, label) if l != -100])\n",
    "            true_labels.append([label_list[l] for p, l in zip(pred, label) if l != -100])\n",
    "\n",
    "print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bd184-7514-45ad-a2d8-2af27c3643aa",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d9dd3-e8b7-43cd-b10a-9614307b2455",
   "metadata": {},
   "source": [
    "### Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394162ea-bb7b-41d3-8087-af873cf007f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.1, 0.5)\n",
    "    batch_size = 64\n",
    "    num_epochs = 3\n",
    "\n",
    "    # Define model with trial's hyperparameters\n",
    "    class BertForNER(nn.Module):\n",
    "        def __init__(self, pretrained_model_name, num_labels):\n",
    "            super(BertForNER, self).__init__()\n",
    "            self.bert = AutoModel.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
    "            self.num_hidden_layers = self.bert.config.num_hidden_layers + 1\n",
    "            self.layer_weights = nn.Parameter(torch.ones(self.num_hidden_layers))\n",
    "            self.dropout = nn.Dropout(dropout_rate)  # Use trial's dropout rate\n",
    "            self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, labels=None):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            hidden_states = torch.stack(outputs.hidden_states, dim=0)  # (num_layers, batch_size, seq_len, hidden_size)\n",
    "            weighted_hidden_states = torch.sum(self.layer_weights[:, None, None, None] * hidden_states, dim=0)\n",
    "            sequence_output = self.dropout(weighted_hidden_states)\n",
    "            logits = self.classifier(sequence_output)\n",
    "\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, logits.size(-1))[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                loss = loss_fn(active_logits, active_labels)\n",
    "\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BertForNER(pretrained_model_name=\"bert-base-cased\", num_labels=num_labels).to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[\"logits\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "                predictions.append([label_list[p] for p, l in zip(pred, label) if l != -100])\n",
    "                true_labels.append([label_list[l] for p, l in zip(pred, label) if l != -100])\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(\n",
    "        [label for seq in true_labels for label in seq],\n",
    "        [label for seq in predictions for label in seq],\n",
    "        average=\"weighted\",\n",
    "    )\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a94854-8507-447d-8909-8ef27d50e0e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)  # Try 20 different combinations\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5147f9b-a11d-4023-b06a-5b40f69ca4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "# Define the final model\n",
    "class BertForFinalNER(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_labels, dropout_rate):\n",
    "        super(BertForFinalNER, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
    "        self.num_hidden_layers = self.bert.config.num_hidden_layers + 1\n",
    "        self.layer_weights = nn.Parameter(torch.ones(self.num_hidden_layers))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = torch.stack(outputs.hidden_states, dim=0)\n",
    "        weighted_hidden_states = torch.sum(self.layer_weights[:, None, None, None] * hidden_states, dim=0)\n",
    "        sequence_output = self.dropout(weighted_hidden_states)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, logits.size(-1))[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss = loss_fn(active_logits, active_labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b847df3-a8cd-4f0c-9511-2e7b520327fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify hyperparameters directly\n",
    "learning_rate = 4.369484270668493e-05\n",
    "dropout_rate = 0.33027803609683487\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1cef0-7165-4731-85a2-432dbe5d991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with specified dropout rate\n",
    "model = BertForFinalNER(pretrained_model_name=\"bert-base-cased\", num_labels=num_labels, dropout_rate=dropout_rate).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "# Define the optimizer with the specified learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare data loaders with the specified batch size\n",
    "train_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Track loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training and validation loops\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch + 1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            logits = outputs[\"logits\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "                predictions.append([label_list[p] for p, l in zip(pred, label) if l != -100])\n",
    "                true_labels.append([label_list[l] for p, l in zip(pred, label) if l != -100])\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0df84-8f8c-4a79-bbc3-77ae4a657c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics\n",
    "flattened_preds = [label for seq in predictions for label in seq]\n",
    "flattened_labels = [label for seq in true_labels for label in seq]\n",
    "\n",
    "accuracy = accuracy_score(flattened_labels, flattened_preds)\n",
    "f1 = f1_score(flattened_labels, flattened_preds, average=\"weighted\")\n",
    "\n",
    "# Print the complete classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(flattened_labels, flattened_preds, target_names=label_list))\n",
    "\n",
    "# Print accuracy and F1 score\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"results/bert/loss_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32766d80-6189-4a47-a077-98e9c13f0e52",
   "metadata": {},
   "source": [
    "## Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96f968-dbcd-4b54-adf9-48af86a6b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = NERDataset(dataset[\"test\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "model.eval()\n",
    "test_predictions, test_true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs[\"logits\"]\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "            test_predictions.append([label_list[p] for p, l in zip(pred, label) if l != -100])\n",
    "            test_true_labels.append([label_list[l] for p, l in zip(pred, label) if l != -100])\n",
    "\n",
    "flattened_test_preds = [label for seq in test_predictions for label in seq]\n",
    "flattened_test_labels = [label for seq in test_true_labels for label in seq]\n",
    "\n",
    "test_accuracy = accuracy_score(flattened_test_labels, flattened_test_preds)\n",
    "test_f1 = f1_score(flattened_test_labels, flattened_test_preds, average=\"weighted\")\n",
    "\n",
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(flattened_test_labels, flattened_test_preds, target_names=label_list))\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
